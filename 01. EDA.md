# Project CAP (Crime Analysis & Prediction)
## Author: Frank Serafine | Log: Exploratory Data Analysis

This logfile contains the various stages of EDA performed on the project prior to Jupyter notebook manipulation and final modeling.  

All modifications made to the imported data in the SQLite database can be found in [the DBMods.sql file](https://github.com/fserafine/ProjectCAP/blob/main/02.%20DBMods-FtWorth.sql) on GitHub.

---
### Stage 1: Download and Assess
---

- Data Origin: US National Incident-Based Reporting System (NIBRS)
- Data Download Source: [FBI Crime Data Explorer](https://crime-data-explorer.fr.cloud.gov/pages/downloads)
    - Subsection: "Crime Incident-Based Data by State"
- State: Texas
- Years: 2010-2019
- Glossary of CSV Column Names: [NIBRS Data Dictionary](https://github.com/fserafine/ProjectCAP/blob/main/NIBRS_DataDictionary.pdf) (Source: FBI)
- Structure of NIBRS Data: [NIBRS Diagram](https://github.com/fserafine/ProjectCAP/blob/main/nibrs_diagram.pdf) (Source: FBI)
    - This informed my creation of a unified table   via a series of targeted JOINs in the SQLite db.

I chose **SQLite** as the medium to store these imported CSVs for initial data wrangling, as there are 43 CSVs for each year of information. Why SQLite instead of MySQL or PostgreSQL? Because SQLite databases are smaller and can import many CSVs as tables all at once without needing to pre-create the table structures manually ahead of time.

The FBI's [NIBRS Readme.md](https://github.com/fserafine/ProjectCAP/blob/main/NIBRS_README.md) file that is included with each year explains that 20 of the 43 CSVs from each year are code lookup tables used for retrieving codes and names and do not change from year to year. The other CSVs that change per year that were essential to this project are:

- `nibrs_incident` (1 crime incident per row)
- `nibrs_offense` (up to 10 offenses per incident)
- `nibrs_offender` (up to 99 offenders per incident)
- `nibrs_victim` (up to 999 victims per incident)
- `nibrs_victim_offense` (maps victims to offenses)
- `nibrs_victim_offender_rel` (relationship info for up to 10 offenders per victim per incident)
- `nibrs_weapon` (up to 3 weapons per offense)
- `nibrs_bias_motivation` (up to 5 bias motivations per offense)
- `agency_participation` (88.5% of TX law enforcement agencies reported to NIBRS between 2010 and 2016, whereas 100% reported to NIBRS from 2017 onward)

The other CSVs present in the downloads were either redundant (categorical information further classifying categorical information) or not essential to the project goal and were not included.

### Stage 2: Import and Preprocess in SQLite
---
Upon importing data from the starting year (2010), a preliminary look by running the following query showed that not every incident had an offense associated with it:
``` sql
SELECT * 
FROM nibrs_incident 
WHERE incident_id 
NOT IN 
    (SELECT incident_id 
    FROM nibrs_offense) 
LIMIT 100
```

To determine why that was, I reviewed the data and found that `cleared_except_id` had a value for each row where no offense was joined, but lacked a value for each row that did join an offense. Adding '`AND cleared_except_id is NULL`' to the `WHERE` clause further returned nothing, confirming that _every incident without an offense associated to it was "cleared by exceptional means,"_ according to the NIBRS Data Dictionary, signifying that **these incidents are not instances of criminal activity**, making them unessential. Going forward, these incidents that lacked an offense were not included.

**Consistency of Information**:

The CSVs from 2010-2016 had the same columns for each year in each CSV. However, the CSVs from 2017 onward differed in that a `data_year` column was present in each. Some of them lacked columns that were present in earlier CSVs, though nothing essential to this project was missing. For example, the nibrs_incident CSVs from 2017-2019 lacked the columns for `ff_line_number`, `incident_number`, and `ddocname`, all of which are described in the NIBRS Data Dictionary as either _unused_ or _internal metadata_, meaningless to this project's objective. 

Because of these CSV differences, it was not possible to import them into the existing tables, so I made the decision to import the main CSVs from 2017-2019 to their own unique tables (with the '_alt' suffix).

In order to ensure that the `ref_race` demographic table isn't referenced by both the victims and the offenders in the same query for the final view I was creating, I had to make some adjustments via SQL. I added a `race` column to the `nibrs_victim`, `nibrs_victim_addl`, `nibrs_offender`, and `nibrs_offender_addl` tables, with the corresponding information from `ref_race` for each row via a JOIN. 

I also formatted the `incident_date` column's values to be a consistent DATE format, as they either came in TIMESTAMP format without any HH:MM:SS data or in a mixed date format of 01-JAN-19. My queries can be found in [the DBMods file](https://github.com/fserafine/ProjectCAP/blob/main/02.%20DBMods-FtWorth.sql).

Upon importing all relevant CSVs, I exported the table to a CSV for basic examination in a Jupyter notebook, but quickly found a problem.

### Stage 3: Dealing With NIBRS Reporting Gaps
---

After basic processing in Jupyter and preliminary modeling, I identified that my test set (2018-2019) experienced highly uncharacteristic increases in crime counts on the order of 200-400%+

I turned to the `agency_participation` table of my database for insight, which contained the list of agencies of Texas and whether or not they had been contributing their incident-based data to NIBRS. I grouped the information by year and counted the amount of distinct agencies that reported to NIBRS versus the amount that did not, and arrived at a percentage of total agencies that were reporting for a given year.

This revealed that __only 5-8% of agencies had actually been reporting prior to 2017__. From 2017 onward, 100% of agencies reported, explaining the uptick in reporting. This was a major problem, leading me to narrow the focus of the project from the entire state of Texas down to just one city that had been reporting consistently for all ten years: **Fort Worth**

### Stage 4: Focus on Fort Worth, TX  
---  

To narrow the focus of the data to Fort Worth, I first generated [a db view](https://github.com/fserafine/ProjectCAP/blob/main/crime_pred_ftworth_dbview.sql) and turned it into a table, from which I ran several specific modification queries (listed in [02. DBMods-FtWorth.sql](https://github.com/fserafine/ProjectCAP/blob/main/02.%20DBMods-FtWorth.sql)) to standardize the data a bit.

At this point, I began experimenting and analyzing the data in [03. CrimePred.ipynb](https://github.com/fserafine/ProjectCAP/blob/main/03.%20CrimePred.ipynb)